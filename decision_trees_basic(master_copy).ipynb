{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees (basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "    - Decision trees are supervised machine learning algorithms used for classification and regression analysis.  \n",
    "    \n",
    "    - Decision trees are used for issues where we have continuous but also categorical input and target features. \n",
    "    \n",
    "    - A decision tree mainly contains of a root node, interior nodes, and leaf nodes which are then connected by branches.\n",
    "    \n",
    "<img src=\"data/images/Decisiontree.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. General method\n",
    "    - Find descriptive features (can be criterion set) which contain the most \"information\" regarding the target feature and then split the dataset along the values of these features such that the target feature values for the resulting sub_datasets are as pure as possible. \n",
    "    - Process of finding the \"most informative\" feature is done until we accomplish a stopping criteria where we then finally end up in so called leaf nodes.\n",
    "    - The leaf nodes contain the predictions we will make for new query instances presented to our trained model. \n",
    "    \n",
    "    \n",
    "3. Advantages of Decision-Tree \n",
    "    * Generate easy rules to understand\n",
    "    * Perform classification without needing much computation\n",
    "    * Indicate which fields are most important for classification\n",
    "\n",
    "\n",
    "4. Disadvantages of Decision-Tree\n",
    "    * Not too suitable for numerical prediction via regression \n",
    "    * Cannot handle cases having many classes and relatively small number of training examples\n",
    "    * Generally computationally expensive to train for very large training examples. The process of growing a decision tree is computationally expensive. \n",
    "    * At each node, each candidate splitting field must be sorted before its best split can be found. In some algorithms, combinations of fields are used and a search must be made for optimal combining weights. Pruning algorithms can also be expensive since many candidate sub-trees must be formed and compared, and then prune appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion = 'gini')\n",
    "# DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of size [n_samples, n_features] \n",
    "# holding the training samples, and an array Y of integer values, size [n_samples], \n",
    "# holding the class labels for the training samples\n",
    "# criterion can either be 'entropy' for information gain or 'gini' for Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Reading in data\n",
    "\n",
    "   * Using zoo data from data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      4\n",
      "3      1\n",
      "4      1\n",
      "5      1\n",
      "6      1\n",
      "7      4\n",
      "8      4\n",
      "9      1\n",
      "10     1\n",
      "11     2\n",
      "12     4\n",
      "13     7\n",
      "14     7\n",
      "15     7\n",
      "16     2\n",
      "17     1\n",
      "18     4\n",
      "19     1\n",
      "20     2\n",
      "21     2\n",
      "22     1\n",
      "23     2\n",
      "24     6\n",
      "25     5\n",
      "26     5\n",
      "27     1\n",
      "28     1\n",
      "29     1\n",
      "      ..\n",
      "71     2\n",
      "72     7\n",
      "73     4\n",
      "74     1\n",
      "75     1\n",
      "76     3\n",
      "77     7\n",
      "78     2\n",
      "79     2\n",
      "80     3\n",
      "81     7\n",
      "82     4\n",
      "83     2\n",
      "84     1\n",
      "85     7\n",
      "86     4\n",
      "87     2\n",
      "88     6\n",
      "89     5\n",
      "90     3\n",
      "91     3\n",
      "92     4\n",
      "93     1\n",
      "94     1\n",
      "95     2\n",
      "96     1\n",
      "97     6\n",
      "98     1\n",
      "99     7\n",
      "100    2\n",
      "Name: class, Length: 101, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# loading in zoo data\n",
    "# dataset = pd.read_csv('data/zoo.data.txt', names=['animal_name','hair','feathers','eggs','milk',\n",
    "#                                                    'airbone','aquatic','predator','toothed','backbone',\n",
    "#                                                   'breathes','venomous','fins','legs','tail','domestic','catsize','class',])\n",
    "dataset = pd.read_csv('zoo_decision_tree.txt', names=['animal_name','hair','feathers','eggs','milk',\n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])\n",
    "dataset.drop(columns = ['animal_name'],axis = 1,inplace=True)\n",
    "\n",
    "x_categories = ['hair','feathers','eggs','milk',\n",
    "                'airbone','aquatic','predator','toothed','backbone',\n",
    "                'breathes','venomous','fins','legs','tail','domestic','catsize']\n",
    "\n",
    "test_classes = {}\n",
    "extract_class_column = dataset['class']\n",
    "print(extract_class_column)\n",
    "\n",
    "for i in range(len(extract_class_column)):\n",
    "    if extract_class_column[i] in test_classes.keys():\n",
    "        test_classes[extract_class_column[i]] += 1\n",
    "    else:\n",
    "        test_classes[extract_class_column[i]] = 1\n",
    "\n",
    "test_classes_names = [item for item in test_classes.keys()]\n",
    "class_name_convert = ['a','d','b','g','f','e','c'] # a - 1, b - 2, c - 3 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data management to split dataset for training and validation\n",
    "\n",
    "    * Available train_test_split function in sklearn.model_selection can do the splitting\n",
    "    \n",
    "      see more information sklearn.model_selection https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data management for setting up training and validation dataset\n",
    "x_train, x_test = train_test_split(dataset.iloc[:,:-1], test_size = 0.20)\n",
    "y_train, y_test = train_test_split(dataset.iloc[:,-1], test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Model-training to build classification model based on decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the trained model from the training step is 72.5% .\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "trained_model = model.fit(x_train,y_train)\n",
    "pred_result_train = trained_model.predict(x_train)\n",
    "y_train = list(y_train)\n",
    "\n",
    "counts = 0\n",
    "for i in range(len(pred_result_train)):\n",
    "    if str(pred_result_train[i]) == str(y_train[i]):\n",
    "        counts += 1\n",
    "accuracy = (counts/len(pred_result_train)) * 100\n",
    "accuracy = round(accuracy,1)\n",
    "\n",
    "print('The accuracy of the trained model from the training step is ' + str(accuracy) + '% .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Model validation to test predictive capability of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the trained model from the validation step is 19.0% .\n"
     ]
    }
   ],
   "source": [
    "# model validation\n",
    "test_results = trained_model.predict(x_test)\n",
    "y_test = list(y_test)\n",
    "counts = 0\n",
    "for i in range(len(test_results)):\n",
    "    if str(test_results[i]) == str(y_test[i]):\n",
    "        counts += 1\n",
    "accuracy = (counts/len(test_results)) * 100\n",
    "accuracy = round(accuracy,1)\n",
    "\n",
    "print('The accuracy of the trained model from the validation step is ' + str(accuracy) + '% .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "import pydot\n",
    "from graphviz import Graph\n",
    "dot_data = tree.export_graphviz(trained_model, out_file=None,\n",
    "                                feature_names=x_categories,\n",
    "                               class_names=class_name_convert,\n",
    "                               filled=True, rounded=True,\n",
    "                               special_characters=False)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
